{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e9f6c4",
   "metadata": {},
   "source": [
    "Im ersten Block erfolgt das Importieren der benötigten Bibliotheken:\n",
    "\n",
    "- Standardbibliothek \"csv\" um eine CSV einzulesen\n",
    "- \"re\"-Bibliothek um unsauberen Text zu bereinigen\n",
    "- mit den \"NLTK\"-Bibliotheken zum Herausfiltern von Stoppwörtern, Vektorisierung mit CountVectorizer, TF-IDF-Vektor und Extrahieren von Themen\n",
    "- \"TextBlob\"-Bibliothek für die Sentiment Analyse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce990a9",
   "metadata": {},
   "source": [
    "Im folgenden Block wird der Datensatz (CSV-Datei) geöffnet und eingelesen. Anschließend wird eine Liste \"no_stopword_list\" erstellt, um die später bereinigten Wörter hinzuzufügen. Ebenso werden drei Zähler erstellt, jeweils um die Sentiment-Reviews zu zählen.\n",
    "\n",
    "In der For-Schleife wird für jede Zeile die Spalte \"Customer Complaint\" ausgewählt und anschließend:\n",
    "\n",
    "- die Sonderzeichen und Satzzeichen mit der \"re\"-Bibliothek bereinigt\n",
    "- alle Buchstaben mit der Funkion \"lower()\" in Kleinbuchstaben umgewandelt\n",
    "- der Text tokenisiert, um einzelne Wörter zu erhalten.\n",
    "\n",
    "In dieser For-Schleife wird auch die Sentiment-Analyse durchgeführt, um den Stimmungs-Wert jedes einzelnen Beschwerde-Eintrags zu erhalten:\n",
    "\n",
    "- Initialisieren des TextBlob-Objekts\n",
    "- Umwandeln des rohen Textes in ein String-Objekt\n",
    "- Durchführung der Sentiment-Analyse\n",
    "- Ausgabe der einzelnen Sentiment-Werte\n",
    "\n",
    "Mit einer If-Verzweigung werden die Reviews eingeordnet und die entsprechenden Zähler hochgezählt.\n",
    "\n",
    "Abschließend wird noch eine Liste erstellt mit englischen Stoppwörtern. Mithilfe einer weiteren For-Schleife werden die einzelnen Wörter untersucht, ob sie Stoppwörter sind. Wenn nicht, dann werden sie an die \"no_stopword_list\" angehängt.\n",
    "\n",
    "Zur Rückmeldung dieses Abschnittes werden die Ergebnisse der Sentiment-Analyse, also die Zähler der drei Stimmungen, ausgegeben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df77f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Comcast.csv', newline='') as csvfile:\n",
    "    raw_text = csv.DictReader(csvfile)\n",
    "\n",
    "    no_stopword_list = []\n",
    "    \n",
    "    negative_count = 0\n",
    "    neutral_count = 0\n",
    "    positive_count = 0\n",
    "\n",
    "    for row in raw_text:\n",
    "        text_list = row['Customer Complaint']\n",
    "        text_list = re.sub('[^a-zA-Z0-9 ]', '', str(text_list))\n",
    "        text_list = text_list.lower()\n",
    "        text_list = text_list.split()\n",
    "\n",
    "        blob = TextBlob(str(text_list))\n",
    "        sentiment = blob.sentiment.polarity\n",
    "        print(\"Sentiment:\", sentiment)\n",
    "\n",
    "        if sentiment < 0:\n",
    "            negative_count += 1\n",
    "        elif sentiment == 0:\n",
    "            neutral_count += 1\n",
    "        else:\n",
    "            positive_count += 1\n",
    "\n",
    "        stop_words_list = set(stopwords.words('english'))\n",
    "        for word in text_list:\n",
    "            if word not in stop_words_list:\n",
    "                no_stopword_list.append(word)\n",
    "\n",
    "    print(\"\\nAnzahl negativer Reviews:\", negative_count)\n",
    "    print(\"Anzahl neutraler Reviews:\", neutral_count)\n",
    "    print(\"Anzahl positiver Reviews:\", positive_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d2fd4",
   "metadata": {},
   "source": [
    "Der folgende Abschnitt wandelt die bereinigten Wörter in numerische Vektoren um. Hier werden zwei Varianten umgesetzt:\n",
    "\n",
    "- CountVectorizer\n",
    "- Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Beide werden jeweils zuerst initialisiert, um dann auf die Wortliste angewandt zu werden,\n",
    "die keine Stoppwörter mehr enthält.\n",
    "\n",
    "Als Rückmeldung werden die einzelnen Vektoren ausgegeben, so können die Ergebnisse dieses Abschnitts verglichen werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a55661",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(no_stopword_list)\n",
    "print(\"CountVectorizer-Ergebnisse:\\n\", vectors)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(no_stopword_list)\n",
    "print(\"\\nTF-IDF-Ergebnisse:\\n\", tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea8072",
   "metadata": {},
   "source": [
    "Im letzten Block wird die Themenmodellierung ausgeführt und die häufigsten Themen extrahiert. Dies soll, basierend auf dem TF-IDF-Vektor auf zwei Arten durchgeführt werden:\n",
    "\n",
    "- Non-Negative Matrix Factorization (NMF)\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Zuerst wird jede Komponente initialisiert und anschließend jeweils auf dem TF-IDF-Vektor angewandt. Die jeweiligen gefundenen Themen werden im Anschluss daran ausgegeben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a081ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=2, init='random', random_state=0)\n",
    "nmf_topic = nmf.fit_transform(tfidf)\n",
    "\n",
    "print(\"\\nThemen, die von NMF gefunden wurden:\")\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Thema %d:\" % (topic_idx))\n",
    "    print(\" \".join([vectorizer.get_feature_names_out()[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=2, max_iter=5, learning_method='online', learning_offset=50., random_state=0)\n",
    "lda_topic = lda.fit_transform(tfidf)\n",
    "\n",
    "print(\"\\nThemen, die von LDA gefunden wurden:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Thema %d:\" % (topic_idx))\n",
    "    print(\" \".join([vectorizer.get_feature_names_out()[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
